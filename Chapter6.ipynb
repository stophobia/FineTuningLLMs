{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f48e02e8"
   },
   "source": [
    "## Chapter 6: Deploying It Locally\n",
    "\n",
    "### Spoilers\n",
    "\n",
    "In this chapter, we will:\n",
    "\n",
    "- Load adapters and merge them to the base model for faster inference\n",
    "- Query the model to generate responses or completions\n",
    "- Convert the fine-tuned model to the GGUF file format used by llama.cpp\n",
    "- Use Ollama and llama.cpp to serve the model through web interfaces and REST APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "245ef1a5"
   },
   "source": [
    "### Setup\n",
    "\n",
    "For better reproducibility during training, use the pinned versions below, the same versions used in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c353825"
   },
   "outputs": [],
   "source": [
    "# Original version\n",
    "# !pip install transformers==4.46.2 peft==0.13.2 accelerate==1.1.1 trl==0.12.1 bitsandbytes==0.45.2 datasets==3.1.0 huggingface-hub==0.26.2 safetensors==0.4.5 pandas==2.2.2 matplotlib==3.8.0 numpy==1.26.4\n",
    "# Updated version - October/2025\n",
    "!pip install transformers==4.56.1 peft==0.17.0 accelerate==1.10.0 trl==0.23.1 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d65f6de"
   },
   "outputs": [],
   "source": [
    "# If you're running on Colab\n",
    "#!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e24a12a"
   },
   "outputs": [],
   "source": [
    "# If you're running on runpod.io's Jupyter Template\n",
    "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aba6a8fd"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd09a846"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import asdict\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM, get_model_status, \\\n",
    "    get_layer_status, prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1228fd66"
   },
   "outputs": [],
   "source": [
    "# If you're running on Colab, you need to download the helper functions' Python file\n",
    "!wget https://raw.githubusercontent.com/dvgodoy/FineTuningLLMs/refs/heads/main/helper_functions.py\n",
    "\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "810fecf6"
   },
   "source": [
    "### The Goal\n",
    "\n",
    "We convert our fine-tuned models and adapters to the GGUF format and then quantize them so they are small enough to run on consumer-grade hardware (without GPUs). Next, we configure and import these models and adapters into Ollama or llama.cpp so we can serve them. That way, we can query them directly in a web interface or using a REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d4d2fca"
   },
   "source": [
    "### The Road So Far\n",
    "\n",
    "****\n",
    "**IMPORTANT UPDATE**: The release of `trl` version 0.20 brought several changes to the `SFTConfig`:\n",
    "- packing is performed differently than it was, unless `packing_strategy='wrapped'` is set;\n",
    "- the `max_seq_length` argument was renamed to `max_length`;\n",
    "- the `bf16` defaults to `True` but, at the time of this update (Aug/2025), it didn't check if the BF16 type was actually available or not, so it's included in the configuration now.\n",
    "\n",
    "Moreover, at the time of writing, the current version of `trl` (0.21) has a known issue where training fails if the LoRA configuration has already been applied to the model, as the trainer freezes the whole model, including the adapters.\n",
    "\n",
    "However, it works as expected when the configuration is passed as the `peft_config` argument to the trainer, since it is applied after freezing the existing layers. \n",
    "\n",
    "If the model already contains the adapters, as in our case, training still works, but we need to use the underlying original model instead (`model.base_model.model`) to ensure the `save_model()` method functions correctly.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "3afd7b7e",
    "outputId": "703433df-5002-403a-e4de-f33c5362e62d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 03:19, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.661000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.631300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From Chapter 2\n",
    "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\n",
    "  \"facebook/opt-350m\", device_map='cuda:0', quantization_config=nf4_config\n",
    ")\n",
    "# From Chapter 3\n",
    "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model_q4, config)\n",
    "\n",
    "# From Chapter 4\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = modify_tokenizer(tokenizer)\n",
    "tokenizer = add_template(tokenizer)\n",
    "\n",
    "peft_model = modify_model(peft_model, tokenizer)\n",
    "\n",
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "\n",
    "# **IMPORTANT UPDATE**: unfortunately, in more recent versions of the `trl` library,\n",
    "# the \"instruction\" format is not properly supported anymore, thus leading to the chat\n",
    "# template not being applied to the dataset. In order to avoid this issue, we can\n",
    "# convert the dataset to the \"conversational\" format.\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns(['prompt', 'completion', 'translation'])\n",
    "\n",
    "# From Chapter 5\n",
    "min_effective_batch_size = 8\n",
    "lr = 3e-4\n",
    "max_seq_length = 64\n",
    "collator_fn = None\n",
    "packing = (collator_fn is None)\n",
    "steps = 20\n",
    "num_train_epochs = 10\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir='./future_name_on_the_hub',\n",
    "    # Dataset\n",
    "    packing=packing,\n",
    "    packing_strategy='wrapped',  # added to approximate original packing behavior\n",
    "    max_length=64,               # renamed in v0.20\n",
    "    # Gradients / Memory\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=min_effective_batch_size,\n",
    "    auto_find_batch_size=True,\n",
    "    # Training\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=lr,\n",
    "    # Env and Logging\n",
    "    report_to='tensorboard',\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=steps,\n",
    "    save_strategy='steps',\n",
    "    save_steps=steps,\n",
    "    bf16=supported               # ensures bf16 (the new default) is only used when it is actually available\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator_fn,\n",
    "    args=sft_config\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('yoda-adapter') # trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ecc5796"
   },
   "source": [
    "### Loading Models and Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2gKM4mDx-20",
    "outputId": "af38c249-0b22-4aba-ac01-56bfe6a570a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (yoda): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (yoda): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (yoda): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (yoda): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(repo_or_folder,\n",
    "                                                 device_map='auto',\n",
    "                                                 adapter_name='yoda')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35f41073"
   },
   "source": [
    "****\n",
    "**IMPORTANT UPDATE**: currently, the embedding layer is only resized if the tokenizer's length exceeds it, thus fixing a long-standing issue that was pointed out in the original version of the book.\n",
    "\n",
    "Due to that issue, one had to load the base model according to the LoRA config, and then combine it with the adapter using the `PeftModel` class, as shown below:\n",
    "\n",
    "```python\n",
    "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
    "config = PeftConfig.from_pretrained(repo_or_folder)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "  config.base_model_name_or_path,\n",
    "  device_map='auto'\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "  base_model,\n",
    "  repo_or_folder,\n",
    "  adapter_name='yoda'\n",
    ")\n",
    "```\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83cda575"
   },
   "outputs": [],
   "source": [
    "model.merge_adapter(['yoda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbf03e85"
   },
   "outputs": [],
   "source": [
    "repo_or_folder = 'dvgodoy/opt-350m-lora-yoda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_or_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "411bc34f",
    "outputId": "1cc5e3b2-b638-4cba-fda8-230aec701f00"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 48,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"model.model.decoder.layers.13.self_attn.q_proj\",\n          \"model.model.decoder.layers.20.self_attn.v_proj\",\n          \"model.model.decoder.layers.13.self_attn.v_proj\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"module_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"lora.Linear\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"enabled\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"active_adapters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"merged_adapters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"requires_grad\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"available_adapters\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"devices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-155d235c-db58-4b8e-be36-bc74bcedc17f\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>module_type</th>\n",
       "      <th>enabled</th>\n",
       "      <th>active_adapters</th>\n",
       "      <th>merged_adapters</th>\n",
       "      <th>requires_grad</th>\n",
       "      <th>available_adapters</th>\n",
       "      <th>devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.model.decoder.layers.0.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.model.decoder.layers.0.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.model.decoder.layers.1.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.model.decoder.layers.1.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.model.decoder.layers.2.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.model.decoder.layers.2.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>model.model.decoder.layers.3.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model.model.decoder.layers.3.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model.model.decoder.layers.4.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>model.model.decoder.layers.4.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>model.model.decoder.layers.5.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>model.model.decoder.layers.5.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>model.model.decoder.layers.6.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>model.model.decoder.layers.6.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model.model.decoder.layers.7.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>model.model.decoder.layers.7.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>model.model.decoder.layers.8.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>model.model.decoder.layers.8.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>model.model.decoder.layers.9.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>model.model.decoder.layers.9.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>model.model.decoder.layers.10.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>model.model.decoder.layers.10.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>model.model.decoder.layers.11.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>model.model.decoder.layers.11.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>model.model.decoder.layers.12.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>model.model.decoder.layers.12.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>model.model.decoder.layers.13.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>model.model.decoder.layers.13.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>model.model.decoder.layers.14.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>model.model.decoder.layers.14.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>model.model.decoder.layers.15.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>model.model.decoder.layers.15.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>model.model.decoder.layers.16.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>model.model.decoder.layers.16.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>model.model.decoder.layers.17.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>model.model.decoder.layers.17.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>model.model.decoder.layers.18.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>model.model.decoder.layers.18.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>model.model.decoder.layers.19.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>model.model.decoder.layers.19.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>model.model.decoder.layers.20.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>model.model.decoder.layers.20.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>model.model.decoder.layers.21.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>model.model.decoder.layers.21.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>model.model.decoder.layers.22.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>model.model.decoder.layers.22.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>model.model.decoder.layers.23.self_attn.v_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>model.model.decoder.layers.23.self_attn.q_proj</td>\n",
       "      <td>lora.Linear</td>\n",
       "      <td>True</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': False}</td>\n",
       "      <td>[yoda]</td>\n",
       "      <td>{'yoda': ['cuda']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-155d235c-db58-4b8e-be36-bc74bcedc17f')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-155d235c-db58-4b8e-be36-bc74bcedc17f button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-155d235c-db58-4b8e-be36-bc74bcedc17f');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-121c82eb-e09c-477a-8931-75f32bc29f5f\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-121c82eb-e09c-477a-8931-75f32bc29f5f')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-121c82eb-e09c-477a-8931-75f32bc29f5f button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_cf935c38-6946-4722-94a2-6a1b10d627b7\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_cf935c38-6946-4722-94a2-6a1b10d627b7 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                              name  module_type  enabled  \\\n",
       "0    model.model.decoder.layers.0.self_attn.v_proj  lora.Linear     True   \n",
       "1    model.model.decoder.layers.0.self_attn.q_proj  lora.Linear     True   \n",
       "2    model.model.decoder.layers.1.self_attn.v_proj  lora.Linear     True   \n",
       "3    model.model.decoder.layers.1.self_attn.q_proj  lora.Linear     True   \n",
       "4    model.model.decoder.layers.2.self_attn.v_proj  lora.Linear     True   \n",
       "5    model.model.decoder.layers.2.self_attn.q_proj  lora.Linear     True   \n",
       "6    model.model.decoder.layers.3.self_attn.v_proj  lora.Linear     True   \n",
       "7    model.model.decoder.layers.3.self_attn.q_proj  lora.Linear     True   \n",
       "8    model.model.decoder.layers.4.self_attn.v_proj  lora.Linear     True   \n",
       "9    model.model.decoder.layers.4.self_attn.q_proj  lora.Linear     True   \n",
       "10   model.model.decoder.layers.5.self_attn.v_proj  lora.Linear     True   \n",
       "11   model.model.decoder.layers.5.self_attn.q_proj  lora.Linear     True   \n",
       "12   model.model.decoder.layers.6.self_attn.v_proj  lora.Linear     True   \n",
       "13   model.model.decoder.layers.6.self_attn.q_proj  lora.Linear     True   \n",
       "14   model.model.decoder.layers.7.self_attn.v_proj  lora.Linear     True   \n",
       "15   model.model.decoder.layers.7.self_attn.q_proj  lora.Linear     True   \n",
       "16   model.model.decoder.layers.8.self_attn.v_proj  lora.Linear     True   \n",
       "17   model.model.decoder.layers.8.self_attn.q_proj  lora.Linear     True   \n",
       "18   model.model.decoder.layers.9.self_attn.v_proj  lora.Linear     True   \n",
       "19   model.model.decoder.layers.9.self_attn.q_proj  lora.Linear     True   \n",
       "20  model.model.decoder.layers.10.self_attn.v_proj  lora.Linear     True   \n",
       "21  model.model.decoder.layers.10.self_attn.q_proj  lora.Linear     True   \n",
       "22  model.model.decoder.layers.11.self_attn.v_proj  lora.Linear     True   \n",
       "23  model.model.decoder.layers.11.self_attn.q_proj  lora.Linear     True   \n",
       "24  model.model.decoder.layers.12.self_attn.v_proj  lora.Linear     True   \n",
       "25  model.model.decoder.layers.12.self_attn.q_proj  lora.Linear     True   \n",
       "26  model.model.decoder.layers.13.self_attn.v_proj  lora.Linear     True   \n",
       "27  model.model.decoder.layers.13.self_attn.q_proj  lora.Linear     True   \n",
       "28  model.model.decoder.layers.14.self_attn.v_proj  lora.Linear     True   \n",
       "29  model.model.decoder.layers.14.self_attn.q_proj  lora.Linear     True   \n",
       "30  model.model.decoder.layers.15.self_attn.v_proj  lora.Linear     True   \n",
       "31  model.model.decoder.layers.15.self_attn.q_proj  lora.Linear     True   \n",
       "32  model.model.decoder.layers.16.self_attn.v_proj  lora.Linear     True   \n",
       "33  model.model.decoder.layers.16.self_attn.q_proj  lora.Linear     True   \n",
       "34  model.model.decoder.layers.17.self_attn.v_proj  lora.Linear     True   \n",
       "35  model.model.decoder.layers.17.self_attn.q_proj  lora.Linear     True   \n",
       "36  model.model.decoder.layers.18.self_attn.v_proj  lora.Linear     True   \n",
       "37  model.model.decoder.layers.18.self_attn.q_proj  lora.Linear     True   \n",
       "38  model.model.decoder.layers.19.self_attn.v_proj  lora.Linear     True   \n",
       "39  model.model.decoder.layers.19.self_attn.q_proj  lora.Linear     True   \n",
       "40  model.model.decoder.layers.20.self_attn.v_proj  lora.Linear     True   \n",
       "41  model.model.decoder.layers.20.self_attn.q_proj  lora.Linear     True   \n",
       "42  model.model.decoder.layers.21.self_attn.v_proj  lora.Linear     True   \n",
       "43  model.model.decoder.layers.21.self_attn.q_proj  lora.Linear     True   \n",
       "44  model.model.decoder.layers.22.self_attn.v_proj  lora.Linear     True   \n",
       "45  model.model.decoder.layers.22.self_attn.q_proj  lora.Linear     True   \n",
       "46  model.model.decoder.layers.23.self_attn.v_proj  lora.Linear     True   \n",
       "47  model.model.decoder.layers.23.self_attn.q_proj  lora.Linear     True   \n",
       "\n",
       "   active_adapters merged_adapters    requires_grad available_adapters  \\\n",
       "0           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "1           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "2           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "3           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "4           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "5           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "6           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "7           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "8           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "9           [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "10          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "11          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "12          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "13          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "14          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "15          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "16          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "17          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "18          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "19          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "20          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "21          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "22          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "23          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "24          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "25          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "26          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "27          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "28          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "29          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "30          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "31          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "32          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "33          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "34          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "35          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "36          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "37          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "38          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "39          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "40          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "41          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "42          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "43          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "44          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "45          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "46          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "47          [yoda]          [yoda]  {'yoda': False}             [yoda]   \n",
       "\n",
       "               devices  \n",
       "0   {'yoda': ['cuda']}  \n",
       "1   {'yoda': ['cuda']}  \n",
       "2   {'yoda': ['cuda']}  \n",
       "3   {'yoda': ['cuda']}  \n",
       "4   {'yoda': ['cuda']}  \n",
       "5   {'yoda': ['cuda']}  \n",
       "6   {'yoda': ['cuda']}  \n",
       "7   {'yoda': ['cuda']}  \n",
       "8   {'yoda': ['cuda']}  \n",
       "9   {'yoda': ['cuda']}  \n",
       "10  {'yoda': ['cuda']}  \n",
       "11  {'yoda': ['cuda']}  \n",
       "12  {'yoda': ['cuda']}  \n",
       "13  {'yoda': ['cuda']}  \n",
       "14  {'yoda': ['cuda']}  \n",
       "15  {'yoda': ['cuda']}  \n",
       "16  {'yoda': ['cuda']}  \n",
       "17  {'yoda': ['cuda']}  \n",
       "18  {'yoda': ['cuda']}  \n",
       "19  {'yoda': ['cuda']}  \n",
       "20  {'yoda': ['cuda']}  \n",
       "21  {'yoda': ['cuda']}  \n",
       "22  {'yoda': ['cuda']}  \n",
       "23  {'yoda': ['cuda']}  \n",
       "24  {'yoda': ['cuda']}  \n",
       "25  {'yoda': ['cuda']}  \n",
       "26  {'yoda': ['cuda']}  \n",
       "27  {'yoda': ['cuda']}  \n",
       "28  {'yoda': ['cuda']}  \n",
       "29  {'yoda': ['cuda']}  \n",
       "30  {'yoda': ['cuda']}  \n",
       "31  {'yoda': ['cuda']}  \n",
       "32  {'yoda': ['cuda']}  \n",
       "33  {'yoda': ['cuda']}  \n",
       "34  {'yoda': ['cuda']}  \n",
       "35  {'yoda': ['cuda']}  \n",
       "36  {'yoda': ['cuda']}  \n",
       "37  {'yoda': ['cuda']}  \n",
       "38  {'yoda': ['cuda']}  \n",
       "39  {'yoda': ['cuda']}  \n",
       "40  {'yoda': ['cuda']}  \n",
       "41  {'yoda': ['cuda']}  \n",
       "42  {'yoda': ['cuda']}  \n",
       "43  {'yoda': ['cuda']}  \n",
       "44  {'yoda': ['cuda']}  \n",
       "45  {'yoda': ['cuda']}  \n",
       "46  {'yoda': ['cuda']}  \n",
       "47  {'yoda': ['cuda']}  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(asdict(layer) for layer in get_layer_status(model))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba40822b",
    "outputId": "151d4b7b-d143-4b42-e92a-8bb8a7d86163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TunerModelStatus(base_model_type='OPTForCausalLM', adapter_model_type='LoraModel', peft_types={'yoda': 'LORA'}, trainable_params=0, total_params=331982848, num_adapter_layers=48, enabled=True, active_adapters=['yoda'], merged_adapters=['yoda'], requires_grad={'yoda': False}, available_adapters=['yoda'], devices={'yoda': ['cuda']})\n"
     ]
    }
   ],
   "source": [
    "print(get_model_status(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a74c72fb",
    "outputId": "9203e887-46ff-4c1f-b5ce-d93b17bcb25e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66fb1c59"
   },
   "source": [
    "### Querying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c663f21f"
   },
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [\n",
    "        {\"role\": \"user\", \"content\": sentence},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(converted_sample,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41066fe7",
    "outputId": "7c2fabd7-5e07-484f-82d6-d9291bb64878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "There is bacon in this sandwich.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = gen_prompt(tokenizer, 'There is bacon in this sandwich.')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97bb4da4"
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt,\n",
    "             max_new_tokens=64,\n",
    "             skip_special_tokens=False,\n",
    "             response_only=False):\n",
    "    # Tokenizes the formatted prompt\n",
    "    tokenized_input = tokenizer(prompt,\n",
    "                                add_special_tokens=False,\n",
    "                                return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    # Generates the response/completion\n",
    "    # if it was trained using mixed precision, uses autocast context\n",
    "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
    "          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
    "    with ctx:    \n",
    "        generation_output = model.generate(**tokenized_input,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # If required, removes the tokens belonging to the prompt\n",
    "    if response_only:\n",
    "        input_length = tokenized_input['input_ids'].shape[1]\n",
    "        generation_output = generation_output[:, input_length:]\n",
    "\n",
    "    # Decodes the tokens back into text\n",
    "    output = tokenizer.batch_decode(generation_output,\n",
    "                                    skip_special_tokens=skip_special_tokens)[0]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4b5b77",
    "outputId": "7c2fe954-edc3-4464-bad2-adad3c922dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "There is bacon in this sandwich.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "In this sandwich, bacon there is.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer,prompt, skip_special_tokens=False, response_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "452a830e",
    "outputId": "7fa2c1cf-1c3a-49d4-892b-930a76a81609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this sandwich, bacon there is.\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer,prompt, skip_special_tokens=True, response_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5b919d66"
   },
   "outputs": [],
   "source": [
    "sentences  = ['There is bacon in this sandwich.', 'Add some cheddar to it.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17acc29f"
   },
   "outputs": [],
   "source": [
    "def batch_generate(model, tokenizer, sentences,\n",
    "             max_new_tokens=64,\n",
    "             skip_special_tokens=False,\n",
    "             response_only=False):\n",
    "\n",
    "    # Converts prompts into conversational format\n",
    "    converted_samples = [[{\"role\": \"user\", \"content\": sentence}]\n",
    "                         for sentence in sentences]\n",
    "\n",
    "    # Applies the chat template to format the prompts\n",
    "    prompts = tokenizer.apply_chat_template(converted_samples,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "\n",
    "    # Forces padding to the left for batch generation\n",
    "    tokenizer.padding_side = 'left'\n",
    "    # Tokenizes the formatted prompts with padding\n",
    "    tokenized_inputs = tokenizer(prompts,\n",
    "                                 padding=True,\n",
    "                                 add_special_tokens=False,\n",
    "                                 return_tensors='pt').to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    # Generates the responses/completions\n",
    "    # if it was trained using mixed precision, uses autocast context\n",
    "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
    "          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
    "    with ctx:   \n",
    "        generation_output = model.generate(**tokenized_inputs,\n",
    "                                           eos_token_id=tokenizer.eos_token_id,\n",
    "                                           pad_token_id=tokenizer.pad_token_id,\n",
    "                                           max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # If required, removes the tokens belonging to the prompts\n",
    "    if response_only:\n",
    "        input_length = tokenized_inputs['input_ids'].shape[1]\n",
    "        generation_output = generation_output[:, input_length:]\n",
    "\n",
    "    # Decodes the tokens back into text\n",
    "    output = tokenizer.batch_decode(generation_output,\n",
    "                                    skip_special_tokens=skip_special_tokens)\n",
    "    return output[0] if isinstance(sentences, str) else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee75635c",
    "outputId": "719769e2-3274-4bdf-be0e-025c5b6e52a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this sandwich, bacon there is.', 'To it, add some cheddar, you must.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_generate(model, tokenizer, sentences, skip_special_tokens=True, response_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dd75791"
   },
   "source": [
    "### Llama.cpp\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp.png?raw=True)\n",
    "\n",
    "<center>Figure 6.1 - Screenshot of llama.cpps GitHub Repo</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07d9754e"
   },
   "source": [
    "#### Converting Adapters\n",
    "\n",
    "In order to convert an adapter to the GGUF format, we need to do the following:\n",
    "\n",
    "- save the adapter to a local folder, either by calling the `save_model()` method after training, as we did in the last chapter, or by downloading it from the Hugging Face Hub (see the aside for details)\n",
    "- clone the llama.cpp repository from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "197d2fad"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fdbc969"
   },
   "source": [
    "- install the `gguf-py` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52c870a1"
   },
   "outputs": [],
   "source": [
    "!pip install llama.cpp/gguf-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJnkIsrbk4uJ"
   },
   "outputs": [],
   "source": [
    "# required in newer versions\n",
    "!pip install mistral-common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9ccc7e0"
   },
   "source": [
    "***\n",
    "**Downloading Models from the Hub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1493a2f"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3d37117"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"dvgodoy/phi3-mini-yoda-adapter\", local_dir='./phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bccb35f0"
   },
   "source": [
    "***\n",
    "\n",
    "- run the `convert_lora_to_gguf.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a621c7be"
   },
   "outputs": [],
   "source": [
    "!python ./llama.cpp/convert_lora_to_gguf.py \\\n",
    "        ./phi3-mini-yoda-adapter \\\n",
    "        --outfile adapter.gguf \\\n",
    "        --outtype q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f13bdca4"
   },
   "source": [
    "- the `outtype` may be one of the following choices: `f32`, `f16`, `bf16`, `q8_0`, or `auto`, which defaults to the highest-fidelity 16-bit float type depending on the first loaded tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f669cfb0"
   },
   "source": [
    "#### Converting Full Models\n",
    "\n",
    "##### Using \"GGUF My Repo\"\n",
    "\n",
    "https://huggingface.co/spaces/ggml-org/gguf-my-repo\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/gguf_my_repo.png?raw=True)\n",
    "\n",
    "<center>Figure 6.2 - Screenshot of \"GGUF My Repo\" space on Hugging Face</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3fa6074"
   },
   "source": [
    "##### Using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b5aa020"
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# Updated  - August/2025\n",
    "!pip install unsloth\n",
    "!pip install protobuf==3.20.1  # this is required at the time of the update\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d16340ec"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained('dvgodoy/phi3-mini-yoda-adapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f55572d7"
   },
   "source": [
    "```\n",
    " Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
    " Unsloth Zoo will now patch everything to make training faster!\n",
    "==((====))==  Unsloth 2025.8.9: Fast Mistral patching. Transformers: 4.55.2.\n",
    "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
    "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
    "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
    " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
    "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
    "\n",
    "model.safetensors:100%\n",
    "2.26G/2.26G[00:30<00:00,202MB/s]\n",
    "generation_config.json:100%\n",
    "194/194[00:00<00:00,17.5kB/s]\n",
    "tokenizer_config.json:\n",
    "3.34k/?[00:00<00:00,202kB/s]\n",
    "tokenizer.model:100%\n",
    "500k/500k[00:00<00:00,1.10MB/s]\n",
    "added_tokens.json:100%\n",
    "293/293[00:00<00:00,21.3kB/s]\n",
    "special_tokens_map.json:100%\n",
    "458/458[00:00<00:00,43.5kB/s]\n",
    "tokenizer.json:\n",
    "1.84M/?[00:00<00:00,6.64MB/s]\n",
    "adapter_model.safetensors:100%\n",
    "50.4M/50.4M[00:00<00:00,60.2MB/s]\n",
    "\n",
    "Unsloth 2025.8.9 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebf0539a"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3f3fa7d"
   },
   "source": [
    "```\n",
    "PeftModelForCausalLM(\n",
    "  (base_model): LoraModel(\n",
    "    (model): MistralForCausalLM(\n",
    "      (model): MistralModel(\n",
    "        (embed_tokens): Embedding(32064, 3072, padding_idx=32009)\n",
    "        (layers): ModuleList(\n",
    "          (0-31): 32 x MistralDecoderLayer(\n",
    "            (self_attn): MistralAttention(\n",
    "              (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "              (k_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "              (v_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "              (o_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.05, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (rotary_emb): LlamaRotaryEmbedding()\n",
    "            )\n",
    "            (mlp): MistralMLP(\n",
    "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
    "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
    "              (down_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.05, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (act_fn): SiLU()\n",
    "            )\n",
    "            (input_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
    "            (post_attention_layernorm): MistralRMSNorm((3072,), eps=1e-05)\n",
    "          )\n",
    "        )\n",
    "        (norm): MistralRMSNorm((3072,), eps=1e-05)\n",
    "      )\n",
    "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdcfe3eb"
   },
   "outputs": [],
   "source": [
    "# This command may fail for several reasons, as it depends on the environment\n",
    "# and the stability of llama.cpp (which is installed during its execution)\n",
    "\n",
    "# Removing the llama.cpp folder we cloned above, so Unsloth can install it on its own\n",
    "!rm -rf llama.cpp/\n",
    "\n",
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "927ce724"
   },
   "source": [
    " ```\n",
    "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
    "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
    "To force `safe_serialization`, set it to `None` instead.\n",
    "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
    "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
    "Unsloth: Will remove a cached repo with size 2.3G\n",
    "\n",
    "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
    "Unsloth: Will use up to 6.0 out of 12.67 RAM for saving.\n",
    "Unsloth: Saving model... This might take 5 minutes ...\n",
    "\n",
    "100%|| 32/32 [00:01<00:00, 24.95it/s]\n",
    "\n",
    "Unsloth: Saving tokenizer... Done.\n",
    "Unsloth: Saving gguf_model/pytorch_model-00001-of-00002.bin...\n",
    "Unsloth: Saving gguf_model/pytorch_model-00002-of-00002.bin...\n",
    "Done.\n",
    "\n",
    "Unsloth: Converting mistral model. Can use fast conversion = True.\n",
    "\n",
    "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
    "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
    "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
    "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
    " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
    "\n",
    "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
    "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
    "Unsloth: [1] Converting model at gguf_model into f16 GGUF format.\n",
    "The output location will be /content/gguf_model/unsloth.F16.gguf\n",
    "This might take 3 minutes...\n",
    "\n",
    "Unsloth: Extending gguf_model/tokenizer.model with added_tokens.json.\n",
    "Originally tokenizer.model is of size (32000).\n",
    "But we need to extend to sentencepiece vocab size (32011).\n",
    "\n",
    "INFO:hf-to-gguf:Loading model: gguf_model\n",
    "INFO:hf-to-gguf:Model architecture: MistralForCausalLM\n",
    "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
    "INFO:hf-to-gguf:Exporting model...\n",
    "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
    "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
    "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 32064}\n",
    "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
    "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
    "...\n",
    "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
    "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
    "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
    "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {3072, 32064}\n",
    "INFO:hf-to-gguf:Set meta model\n",
    "INFO:hf-to-gguf:Set model parameters\n",
    "INFO:hf-to-gguf:gguf: context length = 4096\n",
    "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
    "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
    "INFO:hf-to-gguf:gguf: head count = 32\n",
    "INFO:hf-to-gguf:gguf: key-value head count = 32\n",
    "INFO:hf-to-gguf:gguf: rope theta = 10000.0\n",
    "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
    "INFO:hf-to-gguf:gguf: file type = 1\n",
    "INFO:hf-to-gguf:Set model quantization version\n",
    "INFO:hf-to-gguf:Set model tokenizer\n",
    "INFO:gguf.vocab:Setting special token type bos to 1\n",
    "INFO:gguf.vocab:Setting special token type eos to 32000\n",
    "INFO:gguf.vocab:Setting special token type unk to 0\n",
    "INFO:gguf.vocab:Setting special token type pad to 32009\n",
    "INFO:gguf.vocab:Setting add_bos_token to False\n",
    "INFO:gguf.vocab:Setting add_eos_token to False\n",
    "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
    "' + message['content'] + '<|end|>\n",
    "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
    "' + message['content'] + '<|end|>\n",
    "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
    "' + message['content'] + '<|end|>\n",
    "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
    "' }}{% else %}{{ eos_token }}{% endif %}\n",
    "INFO:gguf.gguf_writer:Writing the following files:\n",
    "INFO:gguf.gguf_writer:/content/gguf_model/unsloth.F16.gguf: n_tensors = 291, total_size = 7.6G\n",
    "Writing: 100%|| 7.64G/7.64G [02:24<00:00, 53.0Mbyte/s]\n",
    "INFO:hf-to-gguf:Model successfully exported to /content/gguf_model/unsloth.F16.gguf\n",
    "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.F16.gguf\n",
    "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
    "main: build = 6230 (30649cab)\n",
    "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
    "main: quantizing '/content/gguf_model/unsloth.F16.gguf' to '/content/gguf_model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
    "llama_model_loader: loaded meta data with 34 key-value pairs and 291 tensors from /content/gguf_model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
    "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
    "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
    "llama_model_loader: - kv   1:                               general.type str              = model\n",
    "llama_model_loader: - kv   2:                               general.name str              = Gguf_Model\n",
    "llama_model_loader: - kv   3:                       general.quantized_by str              = Unsloth\n",
    "llama_model_loader: - kv   4:                         general.size_label str              = 3.8B\n",
    "llama_model_loader: - kv   5:                           general.repo_url str              = https://huggingface.co/unsloth\n",
    "llama_model_loader: - kv   6:                               general.tags arr[str,2]       = [\"unsloth\", \"llama.cpp\"]\n",
    "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
    "llama_model_loader: - kv   8:                       llama.context_length u32              = 4096\n",
    "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
    "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
    "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
    "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
    "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
    "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
    "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 96\n",
    "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 96\n",
    "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
    "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32064\n",
    "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 96\n",
    "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
    "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
    "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
    "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
    "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
    "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
    "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 1\n",
    "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 32000\n",
    "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
    "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 32009\n",
    "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = false\n",
    "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
    "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
    "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
    "llama_model_loader: - type  f32:   65 tensors\n",
    "llama_model_loader: - type  f16:  226 tensors\n",
    "[   1/ 291]                        output.weight - [ 3072, 32064,     1,     1], type =    f16, converting to q6_K .. size =   187.88 MiB ->    77.06 MiB\n",
    "[   2/ 291]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
    "...\n",
    "[ 290/ 291]               blk.31.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
    "[ 291/ 291]                 blk.31.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
    "llama_model_quantize_impl: model size  =  7288.51 MB\n",
    "llama_model_quantize_impl: quant size  =  2210.78 MB\n",
    "\n",
    "main: quantize time = 435560.84 ms\n",
    "main:    total time = 435560.84 ms\n",
    "Unsloth: Conversion completed! Output location: /content/gguf_model/unsloth.Q4_K_M.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b94ddbf5"
   },
   "source": [
    "##### Using Docker Images\n",
    "\n",
    "To convert the model, we need to run the command below:\n",
    "\n",
    "```\n",
    "docker run --rm\n",
    "           -v \"/path/to/saved_model\":/repo\n",
    "           ghcr.io/ggerganov/llama.cpp:full\n",
    "           --convert \"/repo\"\n",
    "           --outtype f32\n",
    "           --outfile /repo/gguf-model-f32.gguf\n",
    "```\n",
    "\n",
    "1. `--rm`: It automatically removes the container from execution after it finishes running, which can be very\n",
    "useful in cases such as ours, where were only interested in running a script once.\n",
    "2. `-v [local path]:[path inside container]`: It maps a folder on your computer to a folder inside the\n",
    "container. This allows the container to \"see\" your local folder as if it were located inside the container\n",
    "itself.\n",
    "3. `[docker image]`: Were using llama.cpps Docker image, ghcr.io/ggerganov/llama.cpp:full\n",
    "4. `--convert [path inside container]`: This is the command were executingit isnt a Docker command,\n",
    "but rather a command thats available in the particular image were using.\n",
    "5. `--outtype [GGUF type]`: This is an argument of the --convert command that specifies the data type of the\n",
    "resulting GGUF file.\n",
    "6. `--outfile [GGUF filename]`: This is yet another argument of the --convert command. It specifies the\n",
    "name of the GGUF file (note that it points to a path inside the container/repowhich was mapped to a\n",
    "folder on your local computer, so in the end, the file is generated directly in your local folder).\n",
    "\n",
    "To quantize the converted model, we need to run the following command:\n",
    "\n",
    "```\n",
    "docker run --rm\n",
    "           -v \"/path/to/saved_model\":/repo\n",
    "           ghcr.io/ggerganov/llama.cpp:full\n",
    "           --quantize \"/repo/gguf-model-f32.gguf\"\n",
    "           \"/repo/gguf-model-Q4_K_M.gguf\"\n",
    "           \"Q4_K_M\"\n",
    "```\n",
    "\n",
    "7. `--quantize [GGUF filename]`: This is the new command were executing, it is a command available in this\n",
    "particular image only, and it should specify which GGUF file is to be quantized (usually the outfile from\n",
    "the convert command).\n",
    "8. `[quantized GGFUF filename]`: the name of the quantized file after the script finishes; make sure to point to\n",
    "the mapped folder so you can access it directly in your local folder as well\n",
    "9. `[quantization type]`: For a full list of quantization types, please check the [documentation](https://github.com/ggerganov/llama.cpp/blob/main/examples/quantize/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f94d1a2b"
   },
   "source": [
    "##### Building llama.cpp\n",
    "\n",
    "```python\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!pip install llama.cpp/gguf-py\n",
    "!pip install -r llama.cpp/requirements.txt\n",
    "```\n",
    "\n",
    "```python\n",
    "!python ./llama.cpp/convert_hf_to_gguf.py /path/to/saved_model --outtype f16\n",
    "```\n",
    "\n",
    "```python\n",
    "!cd llama.cpp && make clean && make\n",
    "```\n",
    "\n",
    "```python\n",
    "!./llama.cpp/quantize\n",
    "    ./path/to/saved_model/ggml-model-f16.gguf\n",
    "    ./path/to/saved_model/ggml-model-q4_0.gguf\n",
    "    q4_0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e120bbcd"
   },
   "source": [
    "### Serving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f76ca5a"
   },
   "source": [
    "#### Ollama\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/ollama.png?raw=True)\n",
    "<center>Figure 6.3 - Screenshot of Ollamas page</center>\n",
    "\n",
    "```\n",
    "ollama run phi3:mini\n",
    "```\n",
    "\n",
    "##### Installing Ollama\n",
    "\n",
    "****\n",
    "**IMPORTANT UPDATE**: at the time of this update (August/2025), the latest versions of Ollama crashed while trying to query a model built from a custom file containing adapters. Version 0.9.6, however, works without any issues.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHuG_yrs5NTS"
   },
   "outputs": [],
   "source": [
    "# !curl -fsSL https://ollama.com/install.sh |  sh\n",
    "# Update - August/2025\n",
    "!curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.9.6 sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1574e2e2"
   },
   "source": [
    "```\n",
    ">>> Installing ollama to /usr/local/bin...\n",
    ">>> Creating ollama user...\n",
    ">>> Adding ollama user to video group...\n",
    ">>> Adding current user to ollama group...\n",
    ">>> Creating ollama systemd service...\n",
    "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
    ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
    ">>> Install complete. Run \"ollama\" from the command line.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcc9f0b5"
   },
   "source": [
    "##### Running Ollama in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "853f2ed6"
   },
   "outputs": [],
   "source": [
    "# Adapter from https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import threading\n",
    "\n",
    "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
    "# Set environment variable for NVIDIA library\n",
    "# Set environment variables for CUDA\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
    "\n",
    "async def run_process(cmd):\n",
    "    print('>>> starting', *cmd)\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    # define an async pipe function\n",
    "    async def pipe(lines):\n",
    "        async for line in lines:\n",
    "            print(line.decode().strip())\n",
    "\n",
    "        await asyncio.gather(\n",
    "            pipe(process.stdout),\n",
    "            pipe(process.stderr),\n",
    "        )\n",
    "\n",
    "    # call it\n",
    "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n",
    "\n",
    "async def start_ollama_serve():\n",
    "    await run_process(['ollama', 'serve'])\n",
    "\n",
    "def run_async_in_thread(loop, coro):\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(coro)\n",
    "    loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75df9604",
    "outputId": "dcf087f2-f47e-42d2-e417-55a02067d34a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> starting"
     ]
    }
   ],
   "source": [
    "# Create a new event loop that will run in a new thread\n",
    "new_loop = asyncio.new_event_loop()\n",
    "\n",
    "# Start ollama serve in a separate thread so the cell won't block execution\n",
    "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee13ef1c"
   },
   "source": [
    "##### Model Files\n",
    "\n",
    "| Instruction |\tDescription |\n",
    "|---|---|\n",
    "|FROM (required) | Defines the base model to use. |\n",
    "|PARAMETER | Sets the parameters for how Ollama will run the model. |\n",
    "|TEMPLATE | The full prompt template to be sent to the model. |\n",
    "|SYSTEM | Specifies the system message that will be set in the template. |\n",
    "|ADAPTER | Defines the (Q)LoRA adapters to apply to the model. |\n",
    "|LICENSE | Specifies the legal license. |\n",
    "|MESSAGE | Specify message history. |\n",
    "\n",
    "```\n",
    "ollama show --modelfile phi3:mini\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "# Modelfile generated by \"ollama show\"\n",
    "# To build a new Modelfile based on this, replace FROM with:\n",
    "# FROM phi3:mini\n",
    "\n",
    "FROM /usr/share/ollama/.ollama/models/blobs/sha256-633fc...\n",
    "TEMPLATE \"{{ if .System }}<|system|>\n",
    "{{ .System }}<|end|>\n",
    "{{ end }}{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\n",
    "PARAMETER stop <|end|>\n",
    "PARAMETER stop <|user|>\n",
    "PARAMETER stop <|assistant|>\n",
    "LICENSE \"\"\"Microsoft.\n",
    "Copyright (c) Microsoft Corporation.\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324,
     "referenced_widgets": [
      "abd08bfba18745b29f496e5967982d54",
      "220aa914c47d4d79a8fa2559858f9bd9",
      "29f85469ace34be9bdd5f6a9d683fbdb",
      "fe8686ff3c5a41729b939f2fe6861e83",
      "73ba4eb0352c4e96979b49c45bf65787",
      "3f2d6574480d4a82b6c97e17cbfab371",
      "359487b900a742d5ba5295cbc5994e35",
      "b8637acf983d47b085f7f852fa8c384f",
      "dae2c1a0c67245669426b172ba0e5585",
      "6b4e541b0bd64b09b4e64a80cce09a51",
      "743e3c6929c5423687dd977e8aa063f5",
      "5f4b1e99bc30439fa50fb555a229a453",
      "1e71d52fce984d8e810aff9ee635a01d",
      "5aa1bb7567324127a27a0ce2fcef97bb",
      "2c9f5dcb505540f1a51548a9119b5f79",
      "db36a69243a94a45b08ed16ab738c4b2",
      "3e5126d9a1b44c6da575a4cd3fe82716",
      "516fad2bef9c4ee8bae4bcb84aa5eca9",
      "a32eda55ab9449c1b905120049ac4deb",
      "9ff460cc9cb04cff8ac0e7035666c58e",
      "02cfb633567e49c984840a9c9c0aa4b6",
      "161dffcad80a4c4aa9610421bdaab9fd",
      "24f02d11268a4337991cbcbc29f2075d",
      "c5e6e9a1059345c285c247273b05bc89",
      "5a03cccee92b48ecaf52b6a58cdc31d3",
      "070beeca88b74b70af8fab9c3ffa5165",
      "9e9ee4459f3c43839629985ddcc90a89",
      "3cc518c3a548416e89512d971f4792f1",
      "cf14527f0a17444b90aacad434eb5bf4",
      "a90187e3c41d417db18275a9f60afd2d",
      "77b2e993cacb4f538de6f8042975e911",
      "d9162703c5504dc090f81a86d215d148",
      "2ff99fbc8e074d799863e1d391c094e5",
      "76c0c3266b2b4c46ade20050a1a59b21",
      "b6a8537d18bd47d08ad8be96719fe704",
      "ced7897debb9403bbd5d2c0a15a7d40b",
      "8a5b0633a1e4400da01860faf13bca08",
      "a9d18a3116f4426a8b5427bcca421118",
      "8708fb0088a14a1cb73a244df389a771",
      "685cee5ff891489a91d3f1683c3283ee",
      "0cf686b37f1e4ca9a4f028b051152af0",
      "b16e36144bac4be7a2169465f45e6af6",
      "2f1af6f01b4644018c51aa5d9f42e139",
      "4613c227d1db48fc8f2a2c5d2fd4ec45",
      "8b442b9b18fb48cba36704565e43ff43",
      "4e11bdfe7a194e949217b31461768bae",
      "bd2a42b1b6134c7d8bcdd2a50c690eae",
      "25ddb02567b047d4a3fb1826a1159718",
      "bf7c85fb5c2f460294ba9fda03e460d4",
      "7bb64117014143e483f9489f8133f478",
      "69e086e12eac463fa8f988c53e6b683c",
      "2269227209a0407e8e70dd39d0485b83",
      "2e3f5e3fcc83467d96a0941ed8164314",
      "7f29f4038d514885bf67efe6e020ada4",
      "ffb434c3e7474c3b82984ef41c46d221"
     ]
    },
    "id": "c03abae0",
    "outputId": "8a3aa6c7-1d05-4225-8902-b54758e3eed1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd08bfba18745b29f496e5967982d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4b1e99bc30439fa50fb555a229a453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f02d11268a4337991cbcbc29f2075d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c0c3266b2b4c46ade20050a1a59b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b442b9b18fb48cba36704565e43ff43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_phi3 = AutoTokenizer.from_pretrained('microsoft/phi-3-mini-4k-instruct')\n",
    "print(tokenizer_phi3.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4012459"
   },
   "source": [
    "##### Importing Models\n",
    "\n",
    "###### Custom (Full) Model File\n",
    "\n",
    "```python\n",
    "modelfile = \"\"\"\n",
    "FROM ./phi3-full-model\n",
    "TEMPLATE \"{{ if .System }}<|system|>\n",
    "{{ .System }}<|end|>\n",
    "{{ end }}{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\n",
    "PARAMETER stop <|end|>\n",
    "PARAMETER stop <|user|>\n",
    "PARAMETER stop <|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "with open('phi3-full-modelfile', 'w') as f:\n",
    "    f.write(modelfile)\n",
    "```\n",
    "\n",
    "```\n",
    "!ollama create our_own_phi3 -f phi3-full-modelfile\n",
    "```\n",
    "\n",
    "```\n",
    "!ollama list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62a52507"
   },
   "source": [
    "###### Custom Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80ec65a7"
   },
   "outputs": [],
   "source": [
    "adapterfile = \"\"\"\n",
    "FROM phi3:mini\n",
    "ADAPTER ./adapter.gguf\n",
    "TEMPLATE \"{{ if .System }}<|system|>\n",
    "{{ .System }}<|end|>\n",
    "{{ end }}{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\n",
    "PARAMETER stop <|end|>\n",
    "PARAMETER stop <|user|>\n",
    "PARAMETER stop <|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "with open('phi3-adapter-file', 'w') as f:\n",
    "    f.write(adapterfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62b5fb88"
   },
   "outputs": [],
   "source": [
    "!ollama create our_own_phi3_adapted -f phi3-adapter-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "336581b7",
    "outputId": "f7d5b48a-dcbd-4db8-a790-8400a4e06107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/08/21 - 14:22:45 | 200 |      39.731s |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/08/21 - 14:22:45 | 200 |    1.833702ms |       127.0.0.1 | GET      \"/api/tags\"\n",
      "NAME                           ID              SIZE      MODIFIED       \n",
      "our_own_phi3_adapted:latest    4aa0c981ee99    2.2 GB    36 seconds ago    \n",
      "phi3:mini                      4f2222927938    2.2 GB    36 seconds ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79a7bf67"
   },
   "source": [
    "##### Querying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42618c42"
   },
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ea0b225"
   },
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "prompt = \"The Force is strong in this one!\"\n",
    "response = ollama.generate(model='our_own_phi3_adapted',\n",
    "                           prompt=prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5189b804",
    "outputId": "64670c3c-e279-4dc7-c482-69d6dcfcfdb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this one, the Force is strong.\n"
     ]
    }
   ],
   "source": [
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62e35608",
    "outputId": "2d41a94b-7af6-47ba-e2d2-3c54c6732385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "The Force is strong in this one!<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "[GIN] 2025/08/21 - 14:24:12 | 200 | 19.741434736s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "In this one, the Force is strong!\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role': 'user', 'content': prompt}]\n",
    "formatted = tokenizer_phi3.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(formatted)\n",
    "\n",
    "response = ollama.generate(model='our_own_phi3_adapted',\n",
    "                           prompt=formatted,\n",
    "                           raw=True)\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d07fb076"
   },
   "source": [
    "#### Llama.cpp\n",
    "\n",
    "Using the full Docker image, one that can be used to convert, quantize, and serve:\n",
    "\n",
    "```\n",
    "docker run -v \"/path/to/saved_model\":/model  \\\n",
    "           -p 8080:8000 \\\n",
    "           ghcr.io/ggerganov/llama.cpp:full \\\n",
    "           --server \\\n",
    "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
    "           --port 8000 \\\n",
    "           --host 0.0.0.0\n",
    "```\n",
    "\n",
    "1. `-v [local path]:[path inside container]`: It maps a folder on your computer to a folder inside the\n",
    "container, so effectively speaking, the container can \"see\" your local folder as if it were located inside the\n",
    "container itself.\n",
    "2. `-p [host port]:[container port]`: It forwards requests sent to a port on the host (e.g., 8080) to a port\n",
    "inside the container (e.g., 8000).\n",
    "3. `[docker image]`: Were using llama.cpps Docker image, ghcr.io/ggerganov/llama.cpp:full.\n",
    "4. `--server`: This is the command were executingits not a Docker command, but rather one thats available\n",
    "within the specific image were utilizing.\n",
    "5. `-m /model/[quantized_qguf_file].qguf`: This is the model were serving.\n",
    "6. `--port [container port]`: This is the port inside the container used to serve the model. It should match\n",
    "the container port specified in the second argument.\n",
    "7. `--host [ip address]`: This is the local IP address used to serve the model.\n",
    "\n",
    "Using a smaller Docker image thats specifically built for serving:\n",
    "\n",
    "```\n",
    "docker run -v \"path/to/saved_model\":/model \\\n",
    "           -p 8080:8000 \\\n",
    "           ghcr.io/ggerganov/llama.cpp:server \\\n",
    "           -m /model/gguf-model-Q4_K_M.gguf \\\n",
    "           --port 8000 \\\n",
    "           --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79666d36"
   },
   "source": [
    "##### Web Interface\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_ui.png?raw=True)\n",
    "<center>Figure 6.4 - Screenshot of llama.cpps web UI</center>\n",
    "\n",
    "If you click on the settings button at the top-right corner, you'll see plenty of parameters you can set, such as temperature:\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch6/llama_cpp_settings.png?raw=True)\n",
    "<center>Figure 6.5 - Screenshot of llama.cpps settings</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be242056"
   },
   "source": [
    "##### REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92ee1432"
   },
   "source": [
    "```python\n",
    "url = 'http://0.0.0.0:8080/completion'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "data = {'prompt': 'There is bacon in this sandwich.',\n",
    "        'n_predict': 128}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa4e2819",
    "scrolled": true
   },
   "source": [
    "```python\n",
    "print(response.json()['content'])\n",
    "```\n",
    "\n",
    "```\n",
    " There is no bacon in this sandwich. This statement is a paradox because it contradicts itself, yet it seems to suggest that the sandwich has both bacon and no bacon at the same time.\n",
    "\n",
    "2. This statement is also a paradox, as it claims that it is a lie that it is lying. If the statement is true, then it is indeed a lie, making it false. But if it is false, then it is not a lie, making it true. This creates a circular reasoning that can't be resolved.\n",
    "\n",
    "3. This statement is a paradox\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbb36d0e"
   },
   "source": [
    "### Thank You!\n",
    "\n",
    "If you have any suggestions, or if you find any errors, please don't hesitate to contact me through [GitHub](https://github.com/dvgodoy), [X](https://x.com/dvgodoy), [BlueSky](https://bsky.app/profile/dvgodoy.bsky.social), or [LinkedIn](https://www.linkedin.com/in/dvgodoy/).\n",
    "\n",
    "If you'd like to receive notifications about new book releases, updates, freebies, and discounts, follow me at:\n",
    "\n",
    "<center><a href=\"https://danielgodoy.gumroad.com/subscribe\">https://danielgodoy.gumroad.com/subscribe</a></center>\n",
    "\n",
    "I'm looking forward to hearing back from you!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02cfb633567e49c984840a9c9c0aa4b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "070beeca88b74b70af8fab9c3ffa5165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9162703c5504dc090f81a86d215d148",
      "placeholder": "",
      "style": "IPY_MODEL_2ff99fbc8e074d799863e1d391c094e5",
      "value": "1.94M/?[00:00&lt;00:00,25.2MB/s]"
     }
    },
    "0cf686b37f1e4ca9a4f028b051152af0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "161dffcad80a4c4aa9610421bdaab9fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e71d52fce984d8e810aff9ee635a01d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e5126d9a1b44c6da575a4cd3fe82716",
      "placeholder": "",
      "style": "IPY_MODEL_516fad2bef9c4ee8bae4bcb84aa5eca9",
      "value": "tokenizer.model:100%"
     }
    },
    "220aa914c47d4d79a8fa2559858f9bd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f2d6574480d4a82b6c97e17cbfab371",
      "placeholder": "",
      "style": "IPY_MODEL_359487b900a742d5ba5295cbc5994e35",
      "value": "tokenizer_config.json:"
     }
    },
    "2269227209a0407e8e70dd39d0485b83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24f02d11268a4337991cbcbc29f2075d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5e6e9a1059345c285c247273b05bc89",
       "IPY_MODEL_5a03cccee92b48ecaf52b6a58cdc31d3",
       "IPY_MODEL_070beeca88b74b70af8fab9c3ffa5165"
      ],
      "layout": "IPY_MODEL_9e9ee4459f3c43839629985ddcc90a89"
     }
    },
    "25ddb02567b047d4a3fb1826a1159718": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f29f4038d514885bf67efe6e020ada4",
      "placeholder": "",
      "style": "IPY_MODEL_ffb434c3e7474c3b82984ef41c46d221",
      "value": "599/599[00:00&lt;00:00,23.2kB/s]"
     }
    },
    "29f85469ace34be9bdd5f6a9d683fbdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8637acf983d47b085f7f852fa8c384f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dae2c1a0c67245669426b172ba0e5585",
      "value": 1
     }
    },
    "2c9f5dcb505540f1a51548a9119b5f79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02cfb633567e49c984840a9c9c0aa4b6",
      "placeholder": "",
      "style": "IPY_MODEL_161dffcad80a4c4aa9610421bdaab9fd",
      "value": "500k/500k[00:00&lt;00:00,1.25MB/s]"
     }
    },
    "2e3f5e3fcc83467d96a0941ed8164314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2f1af6f01b4644018c51aa5d9f42e139": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ff99fbc8e074d799863e1d391c094e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "359487b900a742d5ba5295cbc5994e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cc518c3a548416e89512d971f4792f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e5126d9a1b44c6da575a4cd3fe82716": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f2d6574480d4a82b6c97e17cbfab371": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4613c227d1db48fc8f2a2c5d2fd4ec45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e11bdfe7a194e949217b31461768bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bb64117014143e483f9489f8133f478",
      "placeholder": "",
      "style": "IPY_MODEL_69e086e12eac463fa8f988c53e6b683c",
      "value": "special_tokens_map.json:100%"
     }
    },
    "516fad2bef9c4ee8bae4bcb84aa5eca9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a03cccee92b48ecaf52b6a58cdc31d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90187e3c41d417db18275a9f60afd2d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77b2e993cacb4f538de6f8042975e911",
      "value": 1
     }
    },
    "5aa1bb7567324127a27a0ce2fcef97bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a32eda55ab9449c1b905120049ac4deb",
      "max": 499723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9ff460cc9cb04cff8ac0e7035666c58e",
      "value": 499723
     }
    },
    "5f4b1e99bc30439fa50fb555a229a453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e71d52fce984d8e810aff9ee635a01d",
       "IPY_MODEL_5aa1bb7567324127a27a0ce2fcef97bb",
       "IPY_MODEL_2c9f5dcb505540f1a51548a9119b5f79"
      ],
      "layout": "IPY_MODEL_db36a69243a94a45b08ed16ab738c4b2"
     }
    },
    "685cee5ff891489a91d3f1683c3283ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69e086e12eac463fa8f988c53e6b683c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b4e541b0bd64b09b4e64a80cce09a51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73ba4eb0352c4e96979b49c45bf65787": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "743e3c6929c5423687dd977e8aa063f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76c0c3266b2b4c46ade20050a1a59b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6a8537d18bd47d08ad8be96719fe704",
       "IPY_MODEL_ced7897debb9403bbd5d2c0a15a7d40b",
       "IPY_MODEL_8a5b0633a1e4400da01860faf13bca08"
      ],
      "layout": "IPY_MODEL_a9d18a3116f4426a8b5427bcca421118"
     }
    },
    "77b2e993cacb4f538de6f8042975e911": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bb64117014143e483f9489f8133f478": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f29f4038d514885bf67efe6e020ada4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8708fb0088a14a1cb73a244df389a771": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a5b0633a1e4400da01860faf13bca08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f1af6f01b4644018c51aa5d9f42e139",
      "placeholder": "",
      "style": "IPY_MODEL_4613c227d1db48fc8f2a2c5d2fd4ec45",
      "value": "306/306[00:00&lt;00:00,14.1kB/s]"
     }
    },
    "8b442b9b18fb48cba36704565e43ff43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4e11bdfe7a194e949217b31461768bae",
       "IPY_MODEL_bd2a42b1b6134c7d8bcdd2a50c690eae",
       "IPY_MODEL_25ddb02567b047d4a3fb1826a1159718"
      ],
      "layout": "IPY_MODEL_bf7c85fb5c2f460294ba9fda03e460d4"
     }
    },
    "9e9ee4459f3c43839629985ddcc90a89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ff460cc9cb04cff8ac0e7035666c58e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a32eda55ab9449c1b905120049ac4deb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a90187e3c41d417db18275a9f60afd2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a9d18a3116f4426a8b5427bcca421118": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abd08bfba18745b29f496e5967982d54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_220aa914c47d4d79a8fa2559858f9bd9",
       "IPY_MODEL_29f85469ace34be9bdd5f6a9d683fbdb",
       "IPY_MODEL_fe8686ff3c5a41729b939f2fe6861e83"
      ],
      "layout": "IPY_MODEL_73ba4eb0352c4e96979b49c45bf65787"
     }
    },
    "b16e36144bac4be7a2169465f45e6af6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b6a8537d18bd47d08ad8be96719fe704": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8708fb0088a14a1cb73a244df389a771",
      "placeholder": "",
      "style": "IPY_MODEL_685cee5ff891489a91d3f1683c3283ee",
      "value": "added_tokens.json:100%"
     }
    },
    "b8637acf983d47b085f7f852fa8c384f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "bd2a42b1b6134c7d8bcdd2a50c690eae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2269227209a0407e8e70dd39d0485b83",
      "max": 599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2e3f5e3fcc83467d96a0941ed8164314",
      "value": 599
     }
    },
    "bf7c85fb5c2f460294ba9fda03e460d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5e6e9a1059345c285c247273b05bc89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cc518c3a548416e89512d971f4792f1",
      "placeholder": "",
      "style": "IPY_MODEL_cf14527f0a17444b90aacad434eb5bf4",
      "value": "tokenizer.json:"
     }
    },
    "ced7897debb9403bbd5d2c0a15a7d40b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cf686b37f1e4ca9a4f028b051152af0",
      "max": 306,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b16e36144bac4be7a2169465f45e6af6",
      "value": 306
     }
    },
    "cf14527f0a17444b90aacad434eb5bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9162703c5504dc090f81a86d215d148": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dae2c1a0c67245669426b172ba0e5585": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "db36a69243a94a45b08ed16ab738c4b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe8686ff3c5a41729b939f2fe6861e83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b4e541b0bd64b09b4e64a80cce09a51",
      "placeholder": "",
      "style": "IPY_MODEL_743e3c6929c5423687dd977e8aa063f5",
      "value": "3.44k/?[00:00&lt;00:00,219kB/s]"
     }
    },
    "ffb434c3e7474c3b82984ef41c46d221": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
